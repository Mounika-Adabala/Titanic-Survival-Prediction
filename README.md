# Titanic Survival Prediction ğŸš€  

This project predicts passenger survival on the **Titanic dataset** using data visualization, preprocessing, and a **Random Forest Classifier**.  


---

## ğŸ“‚ Project Structure  

```
Titanic-Survival-Prediction/
â”‚
â”œâ”€â”€ Titanic_EDA.ipynb      # Main Jupyter Notebook (EDA, preprocessing, training, prediction)
â”œâ”€â”€ train.csv              # Training dataset
â”œâ”€â”€ test.csv               # Test dataset
â””â”€â”€ resultfile.csv         # Predictions saved after running the notebook
```

---

## âš¡ Features  

- **Exploratory Data Analysis (EDA)**  
  - Survivors vs Dead distribution plot  
  - Survival by Gender plot  

- **Preprocessing**  
  - Missing value imputation  
  - Feature engineering (Age groups, Title extraction)  
  - Encoding categorical variables (`Sex`, `Embarked`)  

- **Model Training**  
  - Random Forest Classifier  
  - Validation accuracy printed (~83.8%)  

- **Prediction**  
  - Predictions on `test.csv` saved in `resultfile.csv`  

---

## ğŸ› ï¸ Installation  

1. Clone this repository (or download the project folder):  
   ```bash
   git clone https://github.com/your-username/Titanic-Survival-Prediction.git
   cd Titanic-Survival-Prediction
   ```

2. Install dependencies:  
   ```bash
   pip install pandas numpy matplotlib seaborn scikit-learn jupyter
   ```

3. Launch Jupyter Notebook:  
   ```bash
   jupyter notebook
   ```

4. Open **Titanic_EDA.ipynb** and run all cells.  

---

## â–¶ï¸ How to Run  

1. Place `train.csv` and `test.csv` (from Kaggle Titanic dataset) in the project folder.  
2. Run all notebook cells:  
   - You will see **EDA plots**.  
   - Validation accuracy will be printed.  
   - Predictions will be saved as **resultfile.csv**.  

Example output (`resultfile.csv`):  

```
PassengerId,Survived
892,0
893,1
894,0
...
```

---

## ğŸ“Š Example Visualizations  

- **Survivors vs Dead Distribution**  
- **Survival by Gender**  

(Plots appear inside the notebook when you run it.)  

---

## ğŸ¤– Model Used  

- **Random Forest Classifier**  
- Default hyperparameters (can be tuned for better performance)  

---

## ğŸ“Œ Next Steps  

- Try different models (Logistic Regression, XGBoost, etc.)  
- Hyperparameter tuning (GridSearchCV, RandomizedSearchCV)  
- Feature engineering (family size, deck extraction, etc.)  

---

## ğŸ† Accuracy  

- Validation Accuracy: **â‰ˆ 83.8%**  

---

## ğŸ“œ License  

This project is open-source and free to use for learning and practice.  
